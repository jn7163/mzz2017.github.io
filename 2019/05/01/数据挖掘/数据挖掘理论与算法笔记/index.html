
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>《数据挖掘：理论与算法》笔记 | mzz&#39;s blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="莫之章">
    

    
    <meta name="description" content="引言：最近在看袁博男神的数据挖掘网课，整理一些看课程的时候的笔记，先供自己参考。">
<meta name="keywords" content="数据挖掘">
<meta property="og:type" content="article">
<meta property="og:title" content="《数据挖掘：理论与算法》笔记">
<meta property="og:url" content="http://mzz.pub/2019/05/01/数据挖掘/数据挖掘理论与算法笔记/index.html">
<meta property="og:site_name" content="mzz&#39;s blog">
<meta property="og:description" content="引言：最近在看袁博男神的数据挖掘网课，整理一些看课程的时候的笔记，先供自己参考。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190429220406.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190429221241.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190429221504.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190429221636.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190429234841.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430122303.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430122442.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430122746.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430123054.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430124631.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430124809.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430125126.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430125448.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430131950.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430132217.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430132344.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430132532.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430175646.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430211601.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430211605.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501111209.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501112548.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501113518.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/2.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501114612.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501121024.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501122458.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501131918.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501134947.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501135030.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501141034.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501141446.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501171611.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501192444.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501192447.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501192845.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501193046.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501200144.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501200327.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501200412.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501201036.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501201812.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501203505.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501210038.png">
<meta property="og:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501210904.png">
<meta property="og:updated_time" content="2019-05-01T13:13:48.920Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《数据挖掘：理论与算法》笔记">
<meta name="twitter:description" content="引言：最近在看袁博男神的数据挖掘网课，整理一些看课程的时候的笔记，先供自己参考。">
<meta name="twitter:image" content="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190429220406.png">

    
    
    <link rel="icon" href="/img/f.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">
	
	

<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #1E92FB; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

</html>
  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      
<div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="mzz&#39;s blog">mzz&#39;s blog</a></h1>
				<h2 class="blog-motto">　　　　　虽千万人，  吾往矣</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="http://music.mzz.pub">Music Man</a></li>
					
						<li><a href="http://go.mzz.pub">Go语言圣经</a></li>
					
						<li><a href="//g.mzz.pub">Google Mirror</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索">
						<input type="hidden" name="q" value="site:mzz.pub">
					</form>
					
					</li>
				</ul>
			</ul></nav>			
			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2019/05/01/数据挖掘/数据挖掘理论与算法笔记/" title="《数据挖掘：理论与算法》笔记" itemprop="url">《数据挖掘：理论与算法》笔记</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="莫之章" target="_blank" itemprop="author">莫之章</a>
		
  </p><p class="article-time">
    <time datetime="2019-05-01T13:00:00.000Z" itemprop="datePublished"> 发表于 2019-05-01</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#数据预处理"><span class="toc-number">1.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#采样"><span class="toc-number">1.1.</span> <span class="toc-text">采样</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#smote算法"><span class="toc-number">1.1.1.</span> <span class="toc-text">smote算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#描述和可视化"><span class="toc-number">1.2.</span> <span class="toc-text">描述和可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#特征选择-Feature-Selection"><span class="toc-number">1.3.</span> <span class="toc-text">特征选择 Feature Selection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#熵与信息增益"><span class="toc-number">1.3.1.</span> <span class="toc-text">熵与信息增益</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最优subset"><span class="toc-number">1.3.2.</span> <span class="toc-text">最优subset</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#主成分分析（PCA）"><span class="toc-number">1.4.</span> <span class="toc-text">主成分分析（PCA）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#特征提取-Feature-Extraction"><span class="toc-number">1.4.1.</span> <span class="toc-text">特征提取 Feature Extraction</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#特征提取与特征选择"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">特征提取与特征选择</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#根据Variance选择主成分"><span class="toc-number">1.4.2.</span> <span class="toc-text">根据Variance选择主成分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#正文"><span class="toc-number">1.4.3.</span> <span class="toc-text">正文</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#问题"><span class="toc-number">1.4.4.</span> <span class="toc-text">问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#线性判别分析（LDA）"><span class="toc-number">1.5.</span> <span class="toc-text">线性判别分析（LDA）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#从贝叶斯到决策树（classification）"><span class="toc-number">2.</span> <span class="toc-text">从贝叶斯到决策树（classification）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#贝叶斯"><span class="toc-number">2.1.</span> <span class="toc-text">贝叶斯</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#朴素贝叶斯"><span class="toc-number">2.2.</span> <span class="toc-text">朴素贝叶斯</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#独立"><span class="toc-number">2.2.1.</span> <span class="toc-text">独立</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#条件独立"><span class="toc-number">2.2.2.</span> <span class="toc-text">条件独立</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#独立-independent-和不相关-incorrelated"><span class="toc-number">2.2.3.</span> <span class="toc-text">独立(independent)和不相关(incorrelated)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#拉普拉斯平滑"><span class="toc-number">2.2.4.</span> <span class="toc-text">拉普拉斯平滑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tennis-Example"><span class="toc-number">2.2.5.</span> <span class="toc-text">Tennis Example</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Text-Classification-Example"><span class="toc-number">2.2.6.</span> <span class="toc-text">Text Classification Example</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树——数据、规则和树"><span class="toc-number">2.3.</span> <span class="toc-text">决策树——数据、规则和树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ID3算法"><span class="toc-number">2.3.1.</span> <span class="toc-text">ID3算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Overfitting-过学习"><span class="toc-number">2.3.2.</span> <span class="toc-text">Overfitting 过学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pruning-剪枝"><span class="toc-number">2.3.3.</span> <span class="toc-text">pruning 剪枝</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#不合适的分类属性"><span class="toc-number">2.3.4.</span> <span class="toc-text">不合适的分类属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#对于连续型的属性"><span class="toc-number">2.3.5.</span> <span class="toc-text">对于连续型的属性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络"><span class="toc-number">2.4.</span> <span class="toc-text">神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Perceptrons-感知机"><span class="toc-number">2.4.1.</span> <span class="toc-text">Perceptrons 感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Gradient-Descent-梯度下降"><span class="toc-number">2.4.1.1.</span> <span class="toc-text">Gradient Descent 梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#batch-learning"><span class="toc-number">2.4.1.1.1.</span> <span class="toc-text">batch learning</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#stochastic-learning"><span class="toc-number">2.4.1.1.2.</span> <span class="toc-text">stochastic learning</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问题-1"><span class="toc-number">2.4.1.2.</span> <span class="toc-text">问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mutilayer-Perceptron-多层感知机-——-artificial-neural-network-（ANN）"><span class="toc-number">2.4.2.</span> <span class="toc-text">Mutilayer Perceptron 多层感知机 —— artificial neural network （ANN）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Sigmoid-Threshold-Unit"><span class="toc-number">2.4.2.1.</span> <span class="toc-text">Sigmoid Threshold Unit</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Backpropagation-Rule-逆传播算法，BP算法"><span class="toc-number">2.4.3.</span> <span class="toc-text">Backpropagation Rule (逆传播算法，BP算法)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#对于输出神经元"><span class="toc-number">2.4.3.1.</span> <span class="toc-text">对于输出神经元</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#对于隐含层神经元"><span class="toc-number">2.4.3.2.</span> <span class="toc-text">对于隐含层神经元</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#编程实现"><span class="toc-number">2.4.3.3.</span> <span class="toc-text">编程实现</span></a></li></ol></li></ol></li></ol></li></ol>
		
		</div>
		
		<p>引言：最近在看袁博男神的数据挖掘网课，整理一些看课程的时候的笔记，先供自己参考。</p>
<hr>
<a id="more"></a>
<p>课程地址：<a href="http://www.xuetangx.com/courses/course-v1:TsinghuaX+80240372X+sp/about" target="_blank" rel="noopener">http://www.xuetangx.com/courses/course-v1:TsinghuaX+80240372X+sp/about</a></p>
<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><h2 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h2><h3 id="smote算法"><a href="#smote算法" class="headerlink" title="smote算法"></a>smote算法</h3><p>解决非平衡数据问题，即某类数据扩充，类似插值</p>
<h2 id="描述和可视化"><a href="#描述和可视化" class="headerlink" title="描述和可视化"></a>描述和可视化</h2><p>线性代数要学好，比如说均值、中位数、方差的表示公式</p>
<p>数据相关性使用皮尔逊相关系数描述</p>
<p><a href="https://www.zhihu.com/question/19734616" target="_blank" rel="noopener">https://www.zhihu.com/question/19734616</a></p>
<p>高维数据用box plots，parallel coordinates</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190429220406.png" alt="DeepinScreenshot_select-area_20190429220406.png"></p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190429221241.png" alt="DeepinScreenshot_select-area_20190429221241.png"></p>
<p><a href="http://cluster.cis.drexel.edu/~cchen/citespace/" target="_blank" rel="noopener">CiteSpace: visualizing patterns and trends in scientific literature</a></p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190429221504.png" alt="DeepinScreenshot_select-area_20190429221504.png"></p>
<p><strong>Gephi is the leading visualization and exploration software for all kinds of graphs and networks. Gephi is open-source and free.</strong></p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190429221636.png" alt="DeepinScreenshot_select-area_20190429221636.png"></p>
<h2 id="特征选择-Feature-Selection"><a href="#特征选择-Feature-Selection" class="headerlink" title="特征选择 Feature Selection"></a>特征选择 Feature Selection</h2><h3 id="熵与信息增益"><a href="#熵与信息增益" class="headerlink" title="熵与信息增益"></a>熵与信息增益</h3><p>entropy，熵，用来描述数据的不确定性。比方说一堆数据一半是男一半是女，则熵为1；全男全女，熵为0.<br>$$<br>H(X)=-\sum_{i=1}^np(x_i)log_bp(x_i)<br>$$<br><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190429234841.png" alt="DeepinScreenshot_select-area_20190429234841.png"></p>
<p>我们假定相信这组数据的正确性，则直接判断一个人是男是女概率为0.5，熵是1.0：</p>
<p>$H(X)=-0.5<em>log_20.5-0.5</em>log_20.5=1.0$</p>
<p>假设现在获知其抽不抽烟，那么X分为两个群体</p>
<p>$X: {a=\text{“Non-Smoker”}; \text{b=“Smoker”}}$</p>
<p>$H(S|X=a) = -0.8<em>log_20.8-0.2</em>log_20.2=0.7219$</p>
<p>$H(S|X=b) = -0.05<em>log_20.05-0.95</em>log_20.95=0.2864$</p>
<p>已知不抽烟的人占60%，抽烟的人占40%</p>
<p>$H(S|X)=0.6<em>H(S|X=a)+0.4</em>H(S|X=b)=0.5477$</p>
<p>那么信息增益<strong>Information Gain</strong>为：</p>
<p>$Gain(S,X)=H(S)-H(S|X)=0.4523$</p>
<blockquote>
<p>信息增益(Information Gain)：当你知道一个额外的属性时，系统的熵降低的值。也就是这个属性的价值，做决策树的时候要用。</p>
</blockquote>
<h3 id="最优subset"><a href="#最优subset" class="headerlink" title="最优subset"></a>最优subset</h3><ul>
<li><p>分支限界法</p>
<p>准确的最优subset</p>
</li>
</ul>
<p>下面是三种较优subset解法</p>
<ul>
<li><p>Top K Individual Features</p>
<p>分别K个最优属性组成一个K元组</p>
</li>
<li><p>Sequential Forward Selection</p>
<p>一个属性一个属性加</p>
</li>
<li><p>Sequential Backward Selection</p>
<p>一个属性一个属性减</p>
</li>
</ul>
<p>最优subset实际上是个优化问题，也可以用模拟退火等算法</p>
<h2 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h2><h3 id="特征提取-Feature-Extraction"><a href="#特征提取-Feature-Extraction" class="headerlink" title="特征提取 Feature Extraction"></a>特征提取 Feature Extraction</h3><p>例如三维投射到二维，不同的投射角度造成的信息损失不同。投射即是一种特征提取。</p>
<h4 id="特征提取与特征选择"><a href="#特征提取与特征选择" class="headerlink" title="特征提取与特征选择"></a>特征提取与特征选择</h4><p>特征提取和特征选择是DimensionalityReduction（降维）的两种方法，针对于the curse of dimensionality(维灾难)，都可以达到降维的目的。但是这两个有所不同。</p>
<p>特征提取后的新特征是原来特征的一个映射。</p>
<p>特征选择后的特征是原来特征的一个子集。</p>
<blockquote>
<p>袁博教授说特征提取包括特征选择，也就是说特征选择是特征提取的一种。综合来看我持保持意见。</p>
</blockquote>
<h3 id="根据Variance选择主成分"><a href="#根据Variance选择主成分" class="headerlink" title="根据Variance选择主成分"></a>根据Variance选择主成分</h3><p>Variance（方差）：Variance越大代表Information越多，比方说一组身高数据都是170，那么这组身高数据就是没有太多信息的。</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430122303.png" alt="DeepinScreenshot_select-area_20190430122303.png"></p>
<p>很显然如果要从X1和X2中选一个属性的话，就选X1</p>
<h3 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h3><p>如果是下面这个例子，可以平移和旋转坐标轴：</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430122442.png" alt="DeepinScreenshot_select-area_20190430122442.png"></p>
<p>去均值，于是中心与O重合。然后旋转，此时显然选y1作为主属性。</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430122746.png" alt="DeepinScreenshot_select-area_20190430122746.png"></p>
<p>如何使得S(Y)为对角阵：</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430123054.png" alt="DeepinScreenshot_select-area_20190430123054.png"></p>
<p>而Q是个正交矩阵，所以它的转置等于它的逆矩阵。</p>
<p>$QDQ^T$就是$XX^T$矩阵的一个特征分解。</p>
<p>所以PCA就是做一个坐标轴旋转，把数据之间的correlation去除。经过数学推导，得知所需要的旋转矩阵P就是Q的一个转置。</p>
<p>另一种证明方法：</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430124631.png" alt="DeepinScreenshot_select-area_20190430124631.png"></p>
<p>用拉格朗日乘数法解决：</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430124809.png" alt="DeepinScreenshot_select-area_20190430124809.png"></p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430125126.png" alt="DeepinScreenshot_select-area_20190430125126.png"></p>
<p>可以看到最大化的是特征值$\lambda$，选择特征值最大的1到N-1个特征，即可降维到1到N-1维。</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430125448.png" alt="DeepinScreenshot_select-area_20190430125448.png"></p>
<p>用matlab挑选特征值最大的一个，即1.8，画出在该维度的histogram。</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>PCA是个unsupervision learning，不考虑label，因此在做分类的时候会不好受</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430131950.png" alt="DeepinScreenshot_select-area_20190430131950.png"></p>
<h2 id="线性判别分析（LDA）"><a href="#线性判别分析（LDA）" class="headerlink" title="线性判别分析（LDA）"></a>线性判别分析（LDA）</h2><p>针对有标签的数据，应当用LDA。</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430132217.png" alt="DeepinScreenshot_select-area_20190430132217.png"></p>
<p>投射方向不同，得到的结果差异很大。LDA基本思路是将不同类别的数据尽可能地分开。形象来说，不同class之间距离尽可能远，同一个class数据尽可能紧凑。</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430132344.png" alt="DeepinScreenshot_select-area_20190430132344.png"></p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430132532.png" alt="DeepinScreenshot_select-area_20190430132532.png"></p>
<p>推导：</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430175646.png" alt="DeepinScreenshot_select-area_20190430175646.png"></p>
<blockquote>
<p>因为只有方阵才可能有特征值<br>所以面对长方阵的时候，使用原矩阵转置成原矩阵，两者相乘就变成了方阵，就可以求特征值了<br>这样得到的矩阵是一个对称矩阵</p>
<p><a href="https://blog.csdn.net/weixin_36149892/article/details/86774553" target="_blank" rel="noopener">https://blog.csdn.net/weixin_36149892/article/details/86774553</a></p>
</blockquote>
<blockquote>
<p>特征值和特征向量的物理意义</p>
<p><a href="https://baijiahao.baidu.com/s?id=1595375146698848604&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">https://baijiahao.baidu.com/s?id=1595375146698848604&amp;wfr=spider&amp;for=pc</a></p>
<p>顾名思义，特征值和特征向量表达了一个线性变换的特征。在物理意义上，一个高维空间的线性变换可以想象是在对一个向量在各个方向上进行了不同程度的变换，而特征向量之间是线性无关的，它们对应了最主要的变换方向，同时特征值表达了相应的变换程度。</p>
<p>具体的说，求特征向量，就是把矩阵A所代表的空间进行正交分解，使得A的向量集合可以表示为每个向量a在各个特征向量上的投影长度。<strong>我们通常求特征值和特征向量即为求出这个矩阵能使哪些向量只发生拉伸，而方向不发生变化，观察其发生拉伸的程度。这样做的意义在于，看清一个矩阵在哪些方面能产生最大的分散度（scatter），减少重叠，意味着更多的信息被保留下来。</strong></p>
</blockquote>
<blockquote>
<p>二阶矩阵和向量的乘法（其实就是矩阵和矩阵相乘）</p>
<p><a href="https://wenku.baidu.com/view/c4b49f0e24c52cc58bd63186bceb19e8b9f6ec61.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/c4b49f0e24c52cc58bd63186bceb19e8b9f6ec61.html</a></p>
</blockquote>
<blockquote>
<p>w和x都是向量，那么$w^Tx$即为x向量在w向量上的投影点到O点的距离×|w|</p>
<p>其中w即P64的那条投影直线</p>
</blockquote>
<p>下面求最大值。</p>
<p>先求导：</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430211601.png" alt="DeepinScreenshot_select-area_20190430211601.png"></p>
<p>那么w是特征向量，J是特征值。问题转化为求使得特征值最大的特征向量。</p>
<p>当然这个问题有另一个解法：</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190430211605.png" alt="DeepinScreenshot_select-area_20190430211605.png"></p>
<h1 id="从贝叶斯到决策树（classification）"><a href="#从贝叶斯到决策树（classification）" class="headerlink" title="从贝叶斯到决策树（classification）"></a>从贝叶斯到决策树（classification）</h1><p>分类（有监督）</p>
<h2 id="贝叶斯"><a href="#贝叶斯" class="headerlink" title="贝叶斯"></a>贝叶斯</h2><p>有两个耳熟能详的式子：<br>$$<br>P(A\cup B)=P(A)+P(B)-P(A\cap B)\<br>P(A\cap B)=P(A|B)P(B)=P(B|A)P(A)<br>$$<br>贝叶斯定理：<br>$$<br>P(A|B)=\frac {P(B|A)P(A)}{P(B)}<br>$$</p>
<h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><blockquote>
<p>联合概率分布，边缘概率，条件概率等</p>
<p><a href="https://blog.csdn.net/tick_tock97/article/details/79885868" target="_blank" rel="noopener">https://blog.csdn.net/tick_tock97/article/details/79885868</a></p>
</blockquote>
<p>朴素（naïve），假定联合概率分布的各变量均条件独立，即当$\omega_i$发生时，$\alpha_j$发生与否是独立的，不会相互影响。这是一个很强的假设，所以说它naïve</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501111209.png" alt="DeepinScreenshot_select-area_20190501111209.png"></p>
<h3 id="独立"><a href="#独立" class="headerlink" title="独立"></a>独立</h3><p>$$<br>本身有P(A\cap B)=P(A)P(B|A)\<br>加上条件P(B|A)=P(B)\<br>于是P(A\cap B)=P(A)P(B)<br>$$</p>
<h3 id="条件独立"><a href="#条件独立" class="headerlink" title="条件独立"></a>条件独立</h3><p>G发生时AB独立，<strong>G不发生时A和B可能不独立</strong><br>$$<br>P(A,B|G)=P(A|G)P(B|G)\<br>P(A|G,B)=P(A|G)\<br>P(A,B|G)=P(A,B,G)/P(G)=P(A|B,G)\times P(B,G)/P(G)=P(A|B,G)\times P(B|G)<br>$$<br><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501112548.png" alt="DeepinScreenshot_select-area_20190501112548.png"></p>
<p>看似cancer和male/female是不独立的，但是他们其实是条件独立的。加入了smoking后，Cancer和Male/Female独立。</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501113518.png" alt="DeepinScreenshot_select-area_20190501113518.png"></p>
<p>可以看出A和B不独立，但加入了先验C后AB独立。</p>
<h3 id="独立-independent-和不相关-incorrelated"><a href="#独立-independent-和不相关-incorrelated" class="headerlink" title="独立(independent)和不相关(incorrelated)"></a>独立(independent)和不相关(incorrelated)</h3><p><strong>不相关</strong>指的是<strong>不线性相关</strong>，也就是协方差或者Pearson的线性相关系数为0</p>
<p><img src="http://mzzeast.shumsg.cn/2.png" alt="2.png"></p>
<p>图示：XY不相关</p>
<p>但独立一定不相关。</p>
<h3 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h3><p>离散情况：</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501114612.png" alt="DeepinScreenshot_select-area_20190501114612.png"></p>
<p>分子加1，分母加这个属性可能取几种值。比方说该属性描述头发长短，有长发短发，则加2。</p>
<h3 id="Tennis-Example"><a href="#Tennis-Example" class="headerlink" title="Tennis Example"></a>Tennis Example</h3><p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501121024.png" alt></p>
<p>用到了<br>$$<br>P(yes|sunny,cool,high,strong)\=\frac{P(yes,sunny,cool,high,strong)}{P(sunny,cool,high,strong)}\=\frac{P(yes,sunny,cool,high,strong)}{P(yes,sunny,cool,high,strong)+P(no,sunny,cool,high,strong)}<br>$$</p>
<h3 id="Text-Classification-Example"><a href="#Text-Classification-Example" class="headerlink" title="Text Classification Example"></a>Text Classification Example</h3><p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501122458.png" alt="DeepinScreenshot_select-area_20190501122458.png"></p>
<p>$P(V_K|\omega=\omega_i)=\frac{n_k+1}{n+|Vocabulary|}$即词汇$V_K$在我感兴趣或不感兴趣时，（在这篇文章中）频率是多少</p>
<h2 id="决策树——数据、规则和树"><a href="#决策树——数据、规则和树" class="headerlink" title="决策树——数据、规则和树"></a>决策树——数据、规则和树</h2><p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501131918.png" alt="DeepinScreenshot_select-area_20190501131918.png"></p>
<h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h3><p>为了使得整棵树的高度最小，使用贪心算法，将强属性放在接近树根的位置。</p>
<p>这里涉及到了前面提到的entropy，熵。</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501134947.png" alt="DeepinScreenshot_select-area_20190501134947.png"></p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501135030.png" alt="DeepinScreenshot_select-area_20190501135030.png"></p>
<p>发现District更有效，信息增益更大</p>
<h3 id="Overfitting-过学习"><a href="#Overfitting-过学习" class="headerlink" title="Overfitting 过学习"></a>Overfitting 过学习</h3><p>如果有两个分类器A和B，A在训练集中的误差比B小，但在测试集中B的误差比A小，那么表明A是过学习的。</p>
<h3 id="pruning-剪枝"><a href="#pruning-剪枝" class="headerlink" title="pruning 剪枝"></a>pruning 剪枝</h3><p>合并树梢。</p>
<p>在完整学习后，进行剪枝，看着validation的误差，在误差出现拐点时（前）收手。</p>
<h3 id="不合适的分类属性"><a href="#不合适的分类属性" class="headerlink" title="不合适的分类属性"></a>不合适的分类属性</h3><p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501141034.png" alt="DeepinScreenshot_select-area_20190501141034.png"></p>
<p>引入惩罚，加入SplitInformation来防止分得过于“碎”（一个结点的分支过多）</p>
<h3 id="对于连续型的属性"><a href="#对于连续型的属性" class="headerlink" title="对于连续型的属性"></a>对于连续型的属性</h3><p>将其离散化，选择若干threshold，将其分为n部分。</p>
<p>那么这种threshold好不好，也要用entropy计算。比方说</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501141446.png" alt="DeepinScreenshot_select-area_20190501141446.png"></p>
<p>因此在A这个地方切一刀。</p>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="Perceptrons-感知机"><a href="#Perceptrons-感知机" class="headerlink" title="Perceptrons 感知机"></a>Perceptrons 感知机</h3><p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501171611.png" alt="DeepinScreenshot_select-area_20190501171611.png"></p>
<p>注意，n个输入有n+1个权重</p>
<p>这个判决其实是一个超平面，对输入进行区分。</p>
<h4 id="Gradient-Descent-梯度下降"><a href="#Gradient-Descent-梯度下降" class="headerlink" title="Gradient Descent 梯度下降"></a>Gradient Descent 梯度下降</h4><h5 id="batch-learning"><a href="#batch-learning" class="headerlink" title="batch learning"></a>batch learning</h5><p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501192444.png" alt="DeepinScreenshot_select-area_20190501192444.png"></p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501192447.png" alt="DeepinScreenshot_select-area_20190501192447.png"></p>
<p>在这里做了假设，o(x)=wx，即输出和输入相等（这里的输入指的是net, $net_j=\Sigma w_{ji}x_{ji}$），来取代性质不是很好的阈值函数。</p>
<h5 id="stochastic-learning"><a href="#stochastic-learning" class="headerlink" title="stochastic learning"></a>stochastic learning</h5><p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501192845.png" alt="DeepinScreenshot_select-area_20190501192845.png"></p>
<h4 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h4><p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501193046.png" alt="DeepinScreenshot_select-area_20190501193046.png"></p>
<p>perceptrons无法解决线性不可分问题</p>
<h3 id="Mutilayer-Perceptron-多层感知机-——-artificial-neural-network-（ANN）"><a href="#Mutilayer-Perceptron-多层感知机-——-artificial-neural-network-（ANN）" class="headerlink" title="Mutilayer Perceptron 多层感知机 —— artificial neural network （ANN）"></a>Mutilayer Perceptron 多层感知机 —— artificial neural network （ANN）</h3><p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501200144.png" alt="DeepinScreenshot_select-area_20190501200144.png"></p>
<p>增加一个隐含层，隐含层的功能是做一个映射。</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501200327.png" alt="DeepinScreenshot_select-area_20190501200327.png"></p>
<p>如XOR问题</p>
<p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501200412.png" alt="DeepinScreenshot_select-area_20190501200412.png"></p>
<p>输入经隐含层映射后，用一个简单的perceptron分开即可。</p>
<h4 id="Sigmoid-Threshold-Unit"><a href="#Sigmoid-Threshold-Unit" class="headerlink" title="Sigmoid Threshold Unit"></a>Sigmoid Threshold Unit</h4><p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501201036.png" alt="DeepinScreenshot_select-area_20190501201036.png"></p>
<p>ANN没有使用像阈值开关一样的函数，而是使用了性质很好的sigmoid函数，它的值域是(0,1)，导数在0时最大，且导数为$\sigma(y)\cdot(1-\sigma(y))$，可以很方便地计算出来。下面的算法用到了这个导数性质。</p>
<h3 id="Backpropagation-Rule-逆传播算法，BP算法"><a href="#Backpropagation-Rule-逆传播算法，BP算法" class="headerlink" title="Backpropagation Rule (逆传播算法，BP算法)"></a>Backpropagation Rule (逆传播算法，BP算法)</h3><p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501201812.png" alt="DeepinScreenshot_select-area_20190501201812.png"></p>
<h4 id="对于输出神经元"><a href="#对于输出神经元" class="headerlink" title="对于输出神经元"></a>对于输出神经元</h4><p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501203505.png" alt="DeepinScreenshot_select-area_20190501203505.png"></p>
<p>第$j$个神经元的sigmoid函数的输入是$net_j$而不是$x_{ji}$。很容易理解，因为第$j$个神经元的工作是输入多个值，输出一个值。</p>
<p>所以说根据sigmoid导数的性质，有$\frac {\partial \sigma(net_j)}{\partial net_j}=\sigma(net_j)(\sigma(net_j))=o_j(1-o_j)$</p>
<p>看到红框圈起的公式了吗，它和感知机的公式很像，多了一个$o_j(1-o_j)$，这恰恰也是sigmoid函数的导数。</p>
<p>说回来，感知机中为什么没有，是因为我们假设输入等于输出，即o(x)=wx</p>
<h4 id="对于隐含层神经元"><a href="#对于隐含层神经元" class="headerlink" title="对于隐含层神经元"></a>对于隐含层神经元</h4><p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501210038.png" alt="DeepinScreenshot_select-area_20190501210038.png"></p>
<p>注意，$o_j$和$x_k$相等，而不是说$o_j$和$net_k$相等。</p>
<p>至于为什么最后红框里的公式没有负号，因为$\delta_j$已经自带负号了。</p>
<p>所以，BP算法应该先算输出神经元的误差，然后再算隐含层神经元的误差，这就叫误差逆传播。</p>
<h4 id="编程实现"><a href="#编程实现" class="headerlink" title="编程实现"></a>编程实现</h4><p><img src="http://mzzeast.shumsg.cn/DeepinScreenshot_select-area_20190501210904.png" alt="DeepinScreenshot_select-area_20190501210904.png"></p>
<p>三个公式套一套就好了。</p>
<hr style="margin: 4rem 0 1rem 0; background: linear-gradient(to right, red, orange, yellow, green, blue, indigo, violet);height:.1rem;border:none">

<div align="center" style="font-weight: bold; font-size: 2rem; margin-bottom: 2rem">未完待续</div>  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/数据挖掘/">数据挖掘</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/数据挖掘/">数据挖掘</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="http://mzz.pub/2019/05/01/数据挖掘/数据挖掘理论与算法笔记/" data-title="《数据挖掘：理论与算法》笔记 | mzz&#39;s blog" data-tsina class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev">
 <a href="/2019/05/06/linearAlgebra/linear algebraGilbertStrang/" title="Notes about linear algebra - Gilbert Strang">
  <strong>上一篇：</strong><br>
  <span>
  Notes about linear algebra - Gilbert Strang</span>
</a>
</div>


<div class="next">
<a href="/2019/04/08/JustPlay/rsh/" title="rsh踩坑记">
 <strong>下一篇：</strong><br> 
 <span>rsh踩坑记
</span>
</a>
</div>

</nav>

	




<div id="hypercomments_widget"></div>
<script type="text/javascript">
_hcwp = window._hcwp || [];
_hcwp.push({widget:"Stream", widget_id: 94148});
(function() {
if("HC_LOAD_INIT" in window)return;
HC_LOAD_INIT = true;
var lang = (navigator.language || navigator.systemLanguage || navigator.userLanguage || "en").substr(0, 2).toLowerCase();
var hcc = document.createElement("script"); hcc.type = "text/javascript"; hcc.async = true;
hcc.src = ("https:" == document.location.protocol ? "https" : "http")+"://w.hypercomments.com/widget/hc/94148/"+lang+"/widget.js";
var s = document.getElementsByTagName("script")[0];
s.parentNode.insertBefore(hcc, s.nextSibling);
})();
</script>
<a href="http://hypercomments.com" rel="nofollow" class="hc-link" title="comments widget">comments powered by HyperComments</a>
</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>




  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目录</strong>
 
 <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#数据预处理"><span class="toc-number">1.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#采样"><span class="toc-number">1.1.</span> <span class="toc-text">采样</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#smote算法"><span class="toc-number">1.1.1.</span> <span class="toc-text">smote算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#描述和可视化"><span class="toc-number">1.2.</span> <span class="toc-text">描述和可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#特征选择-Feature-Selection"><span class="toc-number">1.3.</span> <span class="toc-text">特征选择 Feature Selection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#熵与信息增益"><span class="toc-number">1.3.1.</span> <span class="toc-text">熵与信息增益</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最优subset"><span class="toc-number">1.3.2.</span> <span class="toc-text">最优subset</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#主成分分析（PCA）"><span class="toc-number">1.4.</span> <span class="toc-text">主成分分析（PCA）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#特征提取-Feature-Extraction"><span class="toc-number">1.4.1.</span> <span class="toc-text">特征提取 Feature Extraction</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#特征提取与特征选择"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">特征提取与特征选择</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#根据Variance选择主成分"><span class="toc-number">1.4.2.</span> <span class="toc-text">根据Variance选择主成分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#正文"><span class="toc-number">1.4.3.</span> <span class="toc-text">正文</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#问题"><span class="toc-number">1.4.4.</span> <span class="toc-text">问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#线性判别分析（LDA）"><span class="toc-number">1.5.</span> <span class="toc-text">线性判别分析（LDA）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#从贝叶斯到决策树（classification）"><span class="toc-number">2.</span> <span class="toc-text">从贝叶斯到决策树（classification）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#贝叶斯"><span class="toc-number">2.1.</span> <span class="toc-text">贝叶斯</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#朴素贝叶斯"><span class="toc-number">2.2.</span> <span class="toc-text">朴素贝叶斯</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#独立"><span class="toc-number">2.2.1.</span> <span class="toc-text">独立</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#条件独立"><span class="toc-number">2.2.2.</span> <span class="toc-text">条件独立</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#独立-independent-和不相关-incorrelated"><span class="toc-number">2.2.3.</span> <span class="toc-text">独立(independent)和不相关(incorrelated)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#拉普拉斯平滑"><span class="toc-number">2.2.4.</span> <span class="toc-text">拉普拉斯平滑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tennis-Example"><span class="toc-number">2.2.5.</span> <span class="toc-text">Tennis Example</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Text-Classification-Example"><span class="toc-number">2.2.6.</span> <span class="toc-text">Text Classification Example</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树——数据、规则和树"><span class="toc-number">2.3.</span> <span class="toc-text">决策树——数据、规则和树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ID3算法"><span class="toc-number">2.3.1.</span> <span class="toc-text">ID3算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Overfitting-过学习"><span class="toc-number">2.3.2.</span> <span class="toc-text">Overfitting 过学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pruning-剪枝"><span class="toc-number">2.3.3.</span> <span class="toc-text">pruning 剪枝</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#不合适的分类属性"><span class="toc-number">2.3.4.</span> <span class="toc-text">不合适的分类属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#对于连续型的属性"><span class="toc-number">2.3.5.</span> <span class="toc-text">对于连续型的属性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络"><span class="toc-number">2.4.</span> <span class="toc-text">神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Perceptrons-感知机"><span class="toc-number">2.4.1.</span> <span class="toc-text">Perceptrons 感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Gradient-Descent-梯度下降"><span class="toc-number">2.4.1.1.</span> <span class="toc-text">Gradient Descent 梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#batch-learning"><span class="toc-number">2.4.1.1.1.</span> <span class="toc-text">batch learning</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#stochastic-learning"><span class="toc-number">2.4.1.1.2.</span> <span class="toc-text">stochastic learning</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问题-1"><span class="toc-number">2.4.1.2.</span> <span class="toc-text">问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mutilayer-Perceptron-多层感知机-——-artificial-neural-network-（ANN）"><span class="toc-number">2.4.2.</span> <span class="toc-text">Mutilayer Perceptron 多层感知机 —— artificial neural network （ANN）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Sigmoid-Threshold-Unit"><span class="toc-number">2.4.2.1.</span> <span class="toc-text">Sigmoid Threshold Unit</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Backpropagation-Rule-逆传播算法，BP算法"><span class="toc-number">2.4.3.</span> <span class="toc-text">Backpropagation Rule (逆传播算法，BP算法)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#对于输出神经元"><span class="toc-number">2.4.3.1.</span> <span class="toc-text">对于输出神经元</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#对于隐含层神经元"><span class="toc-number">2.4.3.2.</span> <span class="toc-text">对于隐含层神经元</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#编程实现"><span class="toc-number">2.4.3.3.</span> <span class="toc-text">编程实现</span></a></li></ol></li></ol></li></ol></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/2017暑期集训/" title="2017暑期集训">2017暑期集训<sup>4</sup></a></li>
		  
		
		  
			<li><a href="/categories/Cloud/" title="Cloud">Cloud<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/JAVA/" title="JAVA">JAVA<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/Linux/" title="Linux">Linux<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/VPN/" title="VPN">VPN<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/cpp/" title="cpp">cpp<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/linear-algebra/" title="linear algebra">linear algebra<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/rsh/" title="rsh">rsh<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/前端/" title="前端">前端<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/博客搭建指南/" title="博客搭建指南">博客搭建指南<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/安全/" title="安全">安全<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/应试/" title="应试">应试<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/微信小程序/" title="微信小程序">微信小程序<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/数据挖掘/" title="数据挖掘">数据挖掘<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/算法/" title="算法">算法<sup>11</sup></a></li>
		  
		
		  
			<li><a href="/categories/路由器/" title="路由器">路由器<sup>1</sup></a></li>
		  
		
		</ul>
</div>


  
  <div class="tagcloudlist">
    <p class="asidetitle">标签云</p>
    <div class="tagcloudlist clearfix">
       <a href="/tags/BFS/" style="font-size: 10px;">BFS</a> <a href="/tags/Cloud/" style="font-size: 20px;">Cloud</a> <a href="/tags/DP/" style="font-size: 15px;">DP</a> <a href="/tags/JAVA/" style="font-size: 20px;">JAVA</a> <a href="/tags/OAuth-网络-安全/" style="font-size: 10px;">OAuth 网络 安全</a> <a href="/tags/SG博弈/" style="font-size: 20px;">SG博弈</a> <a href="/tags/VPN/" style="font-size: 15px;">VPN</a> <a href="/tags/cpp/" style="font-size: 10px;">cpp</a> <a href="/tags/linear-algebra/" style="font-size: 10px;">linear algebra</a> <a href="/tags/rsh/" style="font-size: 10px;">rsh</a> <a href="/tags/springmvc/" style="font-size: 10px;">springmvc</a> <a href="/tags/ubuntu/" style="font-size: 10px;">ubuntu</a> <a href="/tags/vue/" style="font-size: 10px;">vue</a> <a href="/tags/前端/" style="font-size: 10px;">前端</a> <a href="/tags/博弈/" style="font-size: 20px;">博弈</a> <a href="/tags/图论/" style="font-size: 20px;">图论</a> <a href="/tags/夹逼思想/" style="font-size: 10px;">夹逼思想</a> <a href="/tags/巴什博弈/" style="font-size: 10px;">巴什博弈</a> <a href="/tags/强连通/" style="font-size: 10px;">强连通</a> <a href="/tags/微信小程序/" style="font-size: 10px;">微信小程序</a> <a href="/tags/拓扑排序/" style="font-size: 10px;">拓扑排序</a> <a href="/tags/排列组合/" style="font-size: 10px;">排列组合</a> <a href="/tags/排序/" style="font-size: 15px;">排序</a> <a href="/tags/数据挖掘/" style="font-size: 10px;">数据挖掘</a> <a href="/tags/数论/" style="font-size: 15px;">数论</a> <a href="/tags/最小生成树/" style="font-size: 15px;">最小生成树</a> <a href="/tags/最短路径/" style="font-size: 10px;">最短路径</a> <a href="/tags/组合数/" style="font-size: 10px;">组合数</a> <a href="/tags/置换群/" style="font-size: 10px;">置换群</a> <a href="/tags/计算几何/" style="font-size: 10px;">计算几何</a> <a href="/tags/计组/" style="font-size: 10px;">计组</a> <a href="/tags/路由器/" style="font-size: 10px;">路由器</a>
    </div>
  </div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="http://alphalrx.cn/" target="_blank" title="lrx">lrx</a>
            
          </li>
        
          <li>
            
            	<a href="http://psyduck.wang/" target="_blank" title="psyduck">psyduck</a>
            
          </li>
        
          <li>
            
            	<a href="http://cubercsl.cn/" target="_blank" title="CSL">CSL</a>
            
          </li>
        
          <li>
            
            	<a href="http://ybmj.github.io/" target="_blank" title="ybmj">ybmj</a>
            
          </li>
        
          <li>
            
            	<a href="http://0ggmr0.cn/" target="_blank" title="GMR">GMR</a>
            
          </li>
        
    </ul>
</div>

</aside>
</div>
<iframe id="neteasemusic" style="position: relative; float:left; margin:2em 0 0 3%" frameborder="no" border="0" marginwidth="0" marginheight="0" height="450px" width="297px" src="//music.163.com/outchain/player?type=0&id=452039124&auto=0&height=450"></iframe>
    </div>
    <footer><div id="footer">
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> I am with little well content, And a little from thee sent <br>
			Is enough, with true intent. To be steadfast friend.</p>
	</section>
	 
	<div class="social-font">
		
		<a href="http://weibo.com/6251750072" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		<a href="https://github.com/mzz2017" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		
		
		
		<a href="mailto:m@mzz.pub" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo" style="color: #fff;">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman" style="color: #fff;">Jacman</a> © 2019 
		
		<a href="/about" target="_blank" title="莫之章" style="color: #fff">莫之章</a>
		
		
		</p>
		
		<script type="text/javascript">
		  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
		  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
		  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
		  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');
		  
		  _st('install','KARTphmFVqNf53QQL7QN','2.0.0');
		</script>
</div>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div style="text-align:center; color:#fff;" class="theme-info">
博客全站共56.4k字 | 总访客数:<span id="busuanzi_value_site_uv"> 
</span></div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    $('#toc.toc-aside').css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(15,260-$(this).scrollTop()));
    o.css("right",30);
  });
  
        getSize();
        if (myWidth >= 1024) {
            c.click();
        }
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
      c.click();
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(50,295-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>











<!-- Analytics Begin -->

<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1263193610'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263193610%26online%3D1%26show%3Dline' type='text/javascript'%3E%3C/script%3E"));</script>







<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

<!-- BAIDU_PUSH Begin -->
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
<!-- BAIDU_PUSH End -->

<!-- MathJax Begin -->
<!-- partial('mathjax') -->
<!-- MathJax End -->

<link rel="stylesheet" href="https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css">

<link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  </body>
</html>
